#+coord_cartesian(xlim = c(0, 100), ylim = c(10, 20))
ggplot(data, aes(x = division_idh , y = log(meat_production_tonnes) )) +
geom_boxplot() +
labs(title="Boxplot división IDH", x = "División IDH", y="Producción de carne[toneladas]") +
theme_bw()
#+coord_cartesian(xlim = c(0, 100), ylim = c(10, 20))
library(stargazer)
model_1 <- lm(log(meat_production_tonnes)~log(hindus_100) ,data=data)
model_2 <- lm(log(meat_production_tonnes)~log(hindus_100)+log(pop2020) ,data=data)
#stargazer(model_1, type="latex", align = TRUE, dep.var.labels = c("log(Producción de carne anual)"),covariate.labels=c("log(Hindúes c/100milHab)"),no.space=TRUE,title="Modelos", out="modelos.htm")
summary(model_2)
#stargazer(attitude)
library(ggplot2)
library(dplyr)
View(data)
library(stargazer)
model_1 <- lm(log(meat_production_tonnes)~log(hindus_100) ,data=data)
model_2 <- lm(log(meat_production_tonnes)~log(hindus_100)+log(pop2020) ,data=data)
stargazer(model_1, type="latex", align = TRUE, dep.var.labels = c("log(Producción de carne anual)"),covariate.labels=c("log(Hindúes c/100milHab)"),no.space=TRUE,title="Modelos", out="modelos.htm")
summary(model_2)
#stargazer(attitude)
any(is.na(data))
library(stargazer)
model_1 <- lm(log(meat_production_tonnes)~log(hindus_100) ,data=data)
model_2 <- lm(log(meat_production_tonnes)~log(hindus_100)+log(pop2020) ,data=data)
stargazer(model_2, type="latex", align = TRUE, dep.var.labels = c("log(Producción de carne anual)"),covariate.labels=c("log(Hindúes c/100milHab)"),no.space=TRUE,title="Modelos", out="modelos.htm")
summary(model_2)
#stargazer(attitude)
library(stargazer)
model_1 <- lm(log(meat_production_tonnes)~log(hindus_100) ,data=data)
model_2 <- lm(log(meat_production_tonnes)~log(hindus_100)+log(pop2020) ,data=data)
stargazer(model_2, type="latex", align = TRUE, dep.var.labels = c("log(Producción de carne anual)"),covariate.labels=c("log(Hindúes_100"),no.space=TRUE,title="Modelos", out="modelos.htm")
library(stargazer)
model_1 <- lm(log(meat_production_tonnes)~log(hindus_100) ,data=data)
model_2 <- lm(log(meat_production_tonnes)~log(hindus_100)+log(pop2020) ,data=data)
stargazer(model_2, type="latex", align = TRUE, dep.var.labels = c("log(Producción de carne anual)"),covariate.labels=c("log(Hindúes_100"),no.space=TRUE,title="Modelos", out="modelo_2.htm")
library(stargazer)
model_1 <- lm(log(meat_production_tonnes)~log(hindus_100) ,data=data)
model_2 <- lm(log(meat_production_tonnes)~log(hindus_100)+log(pop2020) ,data=data)
stargazer(model_2, type="latex", align = TRUE, dep.var.labels = c("log(Producción de carne anual)"),covariate.labels=c("log(Hindúes_100)"),no.space=TRUE,title="Modelos", out="modelo_2.htm")
library(stargazer)
model_1 <- lm(log(meat_production_tonnes)~log(hindus_100) ,data=data)
model_2 <- lm(log(meat_production_tonnes)~log(hindus_100)+log(pop2020) ,data=data)
stargazer(model_2, type="latex", align = TRUE, dep.var.labels = c("log(Producción de carne anual)"),covariate.labels=c("log(Hindúes100)"),no.space=TRUE,title="Modelos", out="modelos.htm")
summary(model_2)
#stargazer(attitude)
library(stargazer)
model_1 <- lm(log(meat_production_tonnes)~log(hindus_100) ,data=data)
model_2 <- lm(log(meat_production_tonnes)~log(hindus_100)+log(pop2020) ,data=data)
stargazer(model_1, type="latex", align = TRUE, dep.var.labels = c("log(Producción de carne anual)"),covariate.labels=c("log(Hindúes100)"),no.space=TRUE,title="Modelos", out="modelo_1.htm")
summary(model_2)
#stargazer(attitude)
library(stargazer)
model_1 <- lm(log(meat_production_tonnes)~log(hindus_100) ,data=data)
model_2 <- lm(log(meat_production_tonnes)~log(hindus_100)+log(Proporción) ,data=data)
library(stargazer)
model_1 <- lm(log(meat_production_tonnes)~log(hindus_100) ,data=data)
model_2 <- lm(log(meat_production_tonnes)~log(hindus_100)+Proporción ,data=data)
model_3 <- lm(log(meat_production_tonnes)~log(hindus_100)+log(pop2020) ,data=data)
stargazer(model_2, type="latex", align = TRUE, dep.var.labels = c("log(Producción de carne anual)"),covariate.labels=c("log(Hindúes100)"),no.space=TRUE,title="Modelos", out="modelo_2.htm")
summary(model_2)
#stargazer(attitude)
library(stargazer)
model_1 <- lm(log(meat_production_tonnes)~log(hindus_100) ,data=data)
model_2 <- lm(log(meat_production_tonnes)~log(hindus_100)+Proporción ,data=data)
model_3 <- lm(log(meat_production_tonnes)~log(hindus_100)+log(pop2020) ,data=data)
stargazer(model_2, type="latex", align = TRUE, dep.var.labels = c("log(Producción de carne anual)"),covariate.labels=c("log(Hindúes100)","PHindú>10"),no.space=TRUE,title="Modelos", out="modelo_2.htm")
summary(model_2)
#stargazer(attitude)
library("readxl")
data <- read_excel("Laboratorio_2_-_202_base_de_datos.xlsx")
#Omitimos valores nulos
data <- na.omit(data)
data <- data[data$o10>0 & data$ytrabaj>0,]
str(data)
data$educ <- as.factor(data$educ)
#Creación nueva var de exp laboral
data$exp_potencial <- data$edad-data$esc-6
#Horas mensuales o10*4 = horas mensuales
data$log_ingreso_hora <- log(data$ytrabaj/(data$o10*4))
#data <- data[data$exp_potencial>0 ,]
library(stargazer)
model_1 <- lm(log_ingreso_hora ~ esc + exp_potencial + I(exp_potencial**2), data)
model_2 <- lm(log_ingreso_hora ~ esc + exp_potencial + I(exp_potencial**2)+ I(exp_potencial**3), data)
stargazer(model_1, model_2,title = "Modelos", out = "modelos.html")
#summary(model_2)
library(stats)
library(olsrr)
library(lmtest)
#Test heterocedasticidad
ols_test_breusch_pagan(model_1)
ols_test_breusch_pagan(model_2)
model_1$fitted.values
library(ggplot2)
ggplot(data, aes(x = model_1$fitted.values , y = model_1$residuals )) + geom_point() + theme_bw() + labs(x = "Experiencia potencial[años]", y = "Residuos") + geom_smooth(method="lm")
ggplot(data, aes(x = model_1$fitted.values , y = model_1$residuals )) + geom_point() + theme_bw() + labs(x = "Valores predichos", y = "Residuos") + geom_smooth(method="lm")
library(corrplot)
correlacion <- cor(data[,c(5,8)])
round(correlacion, digits = 2)
corrplot(correlacion, method = "shade", shade.col = NA, tl.col = "black", tl.srt = 70, addCoef.col = "black")
cor(data$exp_potencial,(data$exp_potencial)**2)
library(stargazer)
model_1 <- lm(log(meat_production_tonnes)~log(hindus_100) ,data=data)
library(ggplot2)
library(dplyr)
meat_supply <- read.csv("data/meat-supply-per-person.csv", check.names = FALSE)
gdp_data <- read.csv("data/gdp-per-person.csv")
population <- read.csv("data/population-by-country.csv")
population_age <- read.csv("data/median-age-country.csv", check.names = FALSE)
religion_data <- read.csv("data/religion-by-country.csv")
urban_pop <- read.csv("data/urban-population.csv",sep = ",",check.names = FALSE, encoding ="UTF-8")
beef_data <- read.csv("data/beef_production.csv",sep = ",",check.names = FALSE, encoding ="UTF-8")
meat_production_data <- read.csv("data/meat-production-tonnes.csv",sep = ",",check.names = FALSE, encoding ="UTF-8")
idh_data <- read.csv("data/idh_world.csv",sep = ",",check.names = FALSE, encoding ="UTF-8")
colnames(beef_data)[4] <- "beef_production"
colnames(beef_data)[1] <- "Pais"
colnames(meat_production_data)[4] <- "meat_production_tonnes"
colnames(meat_production_data)[1] <- "Pais"
beef_production_2018 <- beef_data %>% filter(Year==2018) %>% select(Pais, beef_production)
meat_production_2018 <- meat_production_data %>% filter(Year==2018) %>% select(Pais, meat_production_tonnes)
#Del dataset population seleccionamos sólo la poblacion 2020
colnames(meat_supply)[4] <- "Meat_kg_yr"
meat_supply_per_capita <- meat_supply %>% select(Entity, Year, Meat_kg_yr) %>% filter(Year == 2017)
meat_supply_per_capita <- subset(meat_supply_per_capita, select = -Year)
colnames(meat_supply_per_capita)[2] <- "Meat_kg_2017"
colnames(meat_supply_per_capita)[1] <- "Pais"
gdp <- gdp_data %>% select(country, gdpPerCapita, pop)
colnames(gdp)[1] <- "Pais"
colnames(population_age)[1] <- "Pais"
population_age$pop2020 <- as.integer(population_age$pop2020*1000)
pop_age <- population_age %>% select(Pais, pop2020, MedianAge)
pop <- population %>% select(name,pop2020)
colnames(pop)[1] <- "Pais"
colnames(urban_pop)[1] <- "Pais"
urban_2018 <- urban_pop %>% select(Pais,"2018")
colnames(urban_2018)[2] <- "urb2018"
colnames(religion_data)[1] <- "Pais"
#Arreglo del IDH
colnames(idh_data)[2] <- "Pais"
#Ordenamos los datos de idh y seleccionamos sólo las columnas que nos aportan información sobre cada país
idh_data <- idh_data[order(idh_data$`HDI Rank (2018)`),][24:nrow(idh_data),]
#Filtramos el IDH sólo para el año 2018
idh <- idh_data %>% select(Pais, "2018")
idh$`2018` <- as.numeric(as.character(idh_data$`2018`))
idh <- idh[order(idh$`2018`,decreasing = TRUE),]
idh$division_idh <- cut(idh$`2018`, breaks=c(0,0.549,0.700, 0.801, 1), labels=c("bajo","medio","alto","muy_alto"))
colnames(idh)[2] <- "idh_2018"
data_1 <- merge(pop_age, meat_supply_per_capita, by = "Pais", all.x = TRUE)
data_2 <- merge(data_1, meat_production_2018, by = "Pais", all.x = TRUE)
data_3 <- merge(data_2, urban_2018, by = "Pais", all.x = TRUE)
data_4 <- merge(data_3, select(gdp,Pais,gdpPerCapita), by = "Pais", all.x = TRUE)
data_5 <- merge(data_4, idh, by = "Pais", all.x = TRUE)
data <- merge(data_5, select(religion_data,Pais,chistians,unaffiliated,hindus), by = "Pais", all.x = TRUE)
#data$meat_production_per_capita <- data$meat_production_tonnes/data$pop2020
#data$unaffiliated_100 <- data$unaffiliated/data$pop2020*100000
data <- na.omit(data)
#Creacion hindues,cristianos cada 100k habitantes
data$chistians_100 <- (data$chistians/data$pop2020)*100000
data$unaffiliated_100 <- (data$unaffiliated/data$pop2020)*100000
data$hindus_100 <- (data$hindus/data$pop2020)*100000
g3 <- data %>% ggplot(aes(y = log(meat_production_tonnes) , x = log(hindus_100), colour = division_idh)) +
geom_point(size = 5) +
labs(title="log Producción de carne vs log Hindús cada 100.000 habitantes", y = "log producción de carne", x="log Hindues cada 100.000 hab.")+ theme_bw()
ggsave(plot = g3, dpi = 300,width = 10, height = 6, filename = "meat_production_vs_hindus_100_idh.pdf")
g0 <- data %>% ggplot(aes(y = log(meat_production_tonnes) , x = log(hindus_100))) +
geom_point(size = 4, color="red") +
labs(title="log Producción de carne vs log Hindús cada 100.000 habitantes", y = "log producción de carne", x="log Hindues cada 100.000 hab.")+ theme_bw() + scale_colour_manual(values=c("black","red"))
g0
ggsave(plot = g0, dpi = 300,width = 10, height = 6, filename = "meat_production_vs_hindus_100.pdf")
data$p_hindues <- (data$hindus/data$pop2020)*100
data$Proporción <- ifelse(data$p_hindues>10.0,">10% Hindú","<10% Hindú")
g1 <- data %>% filter(p_hindues>10.0) %>% ggplot(aes(x=Pais,y=log(meat_production_tonnes))) + geom_bar(stat="identity",fill="brown") + coord_flip() + theme_bw() + labs(title="Paises que tienen mas un 10% de población Hindú")
g1
ggsave(plot = g1, width = 10, height = 10, dpi = 300, filename = "pais_vs_hindus_100.pdf")
g2 <- data %>% ggplot(aes(y = log(meat_production_tonnes) , x = log(hindus_100), colour = Proporción)) +
geom_point(size = 4) +
labs(title="log Producción de carne vs log Hindús cada 100.000 habitantes", y = "log producción de carne", x="log Hindues cada 100.000 hab.")+ theme_bw() + scale_colour_manual(values=c("black","red"))
g2
ggsave(plot = g2, dpi = 300,width = 10, height = 6, filename = "meat_production_vs_hindus_100_p.pdf")
g4 <- data %>% ggplot(aes(y = log(meat_production_tonnes) , x = log(Meat_kg_2017), colour = Proporción)) +
geom_point(size = 5) +
labs(title="log Producción de carne vs log Consumo de carne per cápita", y = "log producción de carne", x="log Consumo de carne per cápita")+ theme_bw() + scale_colour_manual(values=c("black","red"))+
geom_text(aes(label=ifelse(p_hindues>=10.0,as.character(Pais),'')),hjust=-0.2, vjust=0.5)
g4
ggsave(plot = g4, dpi = 300, width = 10, height = 6, filename = "meat_production_vs_meat_consumo_per_capita.pdf")
g5 <- data %>% ggplot(aes(y = log(meat_production_tonnes) , x = log(hindus_100), colour = Proporción)) +
geom_point(size = 4) +
labs(title="log Producción de carne vs log Hindús cada 100.000 habitantes", y = "log producción de carne", x="log Hindues cada 100.000 hab.")+ theme_bw() + scale_colour_manual(values=c("black","red"))
g5
ggsave(plot = g5, dpi = 300, width = 10, height = 6, filename = "meat_vs_hindus.pdf")
g6 <- data %>% ggplot(aes(y = log(meat_production_tonnes) , x = gdpPerCapita, colour = Proporción)) +
geom_point(size = 2) +
labs(title="log Producción de carne vs PIB per cápita", y = "log producción de carne", x="PIB per cápita")+ theme_bw() + scale_colour_manual(values=c("black","red"))+
geom_text(aes(label=ifelse(p_hindues>=10.0,as.character(Pais),'')),hjust=-0.5, vjust=0.5)
g6
ggsave(plot = g6, dpi = 300, width = 10, height = 6, filename = "meat_production_vs_PIB.pdf")
data %>% ggplot(aes(y = log(meat_production_tonnes) , x = log(hindus_100), colour = division_idh)) +
geom_point(size = 2) +
labs(title="log Producción de carne vs log Hindús cada 100.000 habitantes", y = "log producción de carne", x="log Hindues cada 100.000 hab.")+ theme_bw()
g7 <- data %>% ggplot(aes(y = log(meat_production_tonnes) , x = log(pop2020), colour = Proporción)) +
geom_point(size=4) +
labs(title="log Producción de carne vs log Población", y = "log Producción de carne[toneladas]", x="log Población") +
theme_bw() +
scale_color_manual(values=c("black","red"))
g7
ggsave(plot = g7, dpi = 300, width = 10, height = 6, filename = "meat_production_vs_pob_p.pdf")
p3 <- data %>% ggplot(aes(x = log(meat_production_tonnes) , y = MedianAge, colour = division_idh )) +
geom_point(size=4) +
labs(title="log producción de carne vs Edad media población", x = "log producción de carne", y="Edad media población")+
theme_bw()
p3
p4 <- data %>% ggplot(aes(x = log(meat_production_tonnes) , y = urb2018, colour = Proporción)) +
geom_point(size=4) +
labs(title="log Producción de carne vs % Urbanización", x = "log producción de carne", y="% Urbanización") +
theme_bw()
p4
data$meat_production_per_capita <- data$meat_production_tonnes/data$pop2020
g6 <- data %>% ggplot(aes(x = log(meat_production_per_capita), y = gdpPerCapita, colour = Proporción)) +
geom_point() +
labs(title="log Producción de carne per cápita vs PIB per cápita", x = "Producción de carne per cápita", y="PIB per cápita") +
theme_bw()
g6
ggsave(plot = g6, dpi = 300, width = 10, height = 6, filename = "meat_production_per_capita_vs_PIB.pdf")
#data %>% ggplot(aes(x = division_idh, y = log(Meat_kg_2017))) + geom_boxplot() + theme_bw()
data %>% ggplot(aes(x = Proporción, y = log(Meat_kg_2017))) + geom_boxplot() + theme_bw()
#ggplot(na.omit(data), aes(x = Meat_kg_2017 , y = reorder(Pais, Meat_kg_2017))) +
#  geom_bar(stat = "identity") +
#  labs(title="Pais vs consumo de carne per cápita al año 2017", x = "Consumo de carne per cápita al año 2017", y="País") +
#  theme_bw()
#+coord_cartesian(xlim = c(0, 100), ylim = c(10, 20))
ggplot(data, aes(x = division_idh , y = log(meat_production_tonnes) )) +
geom_boxplot() +
labs(title="Boxplot división IDH", x = "División IDH", y="Producción de carne[toneladas]") +
theme_bw()
#+coord_cartesian(xlim = c(0, 100), ylim = c(10, 20))
library(stargazer)
model_1 <- lm(log(meat_production_tonnes)~log(hindus_100) ,data=data)
model_2 <- lm(log(meat_production_tonnes)~log(hindus_100)+Proporción ,data=data)
model_3 <- lm(log(meat_production_tonnes)~log(hindus_100)+log(pop2020) ,data=data)
stargazer(model_1,model_2,model_3, type="latex", align = TRUE, dep.var.labels = c("log(Producción de carne anual)"),covariate.labels=c("log(Hindúes100)","PHindú>10"),no.space=TRUE,title="Modelos", out="modelo_2.htm")
summary(model_2)
#stargazer(attitude)
library(stargazer)
model_1 <- lm(log(meat_production_tonnes)~log(hindus_100) ,data=data)
model_2 <- lm(log(meat_production_tonnes)~log(hindus_100)+Proporción ,data=data)
model_3 <- lm(log(meat_production_tonnes)~log(hindus_100)+log(pop2020) ,data=data)
stargazer(model_1,model_2,model_3, type="latex", align = TRUE, dep.var.labels = c("log(Producción de carne anual)"),covariate.labels=c("log(Hindúes100)","PHindú>10","log(pob)"),no.space=TRUE,title="Modelos", out="modelo_2.htm")
summary(model_2)
#stargazer(attitude)
library(stargazer)
model_1 <- lm(log(meat_production_tonnes)~log(hindus_100) ,data=data)
model_2 <- lm(log(meat_production_tonnes)~log(hindus_100)+Proporción ,data=data)
model_3 <- lm(log(meat_production_tonnes)~log(hindus_100)+log(pop2020) ,data=data)
model_4 <- lm(log(meat_production_tonnes)~log(hindus_100)+Proporción+log(pop2020) ,data=data)
stargazer(model_1,model_2,model_3, type="latex", align = TRUE, dep.var.labels = c("log(Producción de carne anual)"),covariate.labels=c("log(Hindúes100)","PHindú>10","log(pob)"),no.space=TRUE,title="Modelos", out="modelo_2.htm")
summary(model_2)
#stargazer(attitude)
library(stargazer)
model_1 <- lm(log(meat_production_tonnes)~log(hindus_100) ,data=data)
model_2 <- lm(log(meat_production_tonnes)~log(hindus_100)+Proporción ,data=data)
model_3 <- lm(log(meat_production_tonnes)~log(hindus_100)+log(pop2020) ,data=data)
model_4 <- lm(log(meat_production_tonnes)~log(hindus_100)+Proporción+log(pop2020) ,data=data)
stargazer(model_1,model_2,model_3,model_4, type="latex", align = TRUE, dep.var.labels = c("log(Producción de carne anual)"),covariate.labels=c("log(Hindúes100)","PHindú>10","log(pob)"),no.space=TRUE,title="Modelos", out="modelo_2.htm")
summary(model_2)
#stargazer(attitude)
library(stargazer)
model_1 <- lm(log(meat_production_tonnes)~log(hindus_100) ,data=data)
model_2 <- lm(log(meat_production_tonnes)~log(hindus_100)+Proporción ,data=data)
model_3 <- lm(log(meat_production_tonnes)~log(hindus_100)+log(pop2020) ,data=data)
model_4 <- lm(log(meat_production_tonnes)~log(hindus_100)+Proporción+log(pop2020) ,data=data)
stargazer(model_1,model_2,model_3,model_4, type="latex", align = TRUE, dep.var.labels = c("log(Producción de carne anual)"),covariate.labels=c("log(Hindúes100)","PHindú>10","log(pob)"),no.space=TRUE, omit.stat = c("f","ser"), title="Modelos", out="modelo_2.htm")
summary(model_2)
#stargazer(attitude)
#Palabras mas repetidas en comentarios neutrales
neu_tweets <- tweets_expand %>%
filter(Sentiment == "Neutral") %>%
count(word)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidytext)
library(RColorBrewer)
library(wordcloud)
library(wordcloud2)
train  <- read.csv("data/Corona_NLP_train.csv", encoding="Latin-1")
test <- read.csv("data/Corona_NLP_test.csv", encoding="Latin-1")
df <- rbind(train,test)
head(df)
df$TweetAt <- as.Date(df$TweetAt, format="%d-%m-%y")
df$OriginalTweet <- as.character(df$OriginalTweet)
df$Location <- as.character(df$Location)
str(df)
summary(df$TweetAt)
ggplot(data=df, aes(x=TweetAt)) + geom_histogram(position="identity", bins=30) +
labs(title = "Distribución de las fechas de tweets", x = "fecha de publicación",
y = "número de tweets") + theme_bw()
tweets_mes_dia <- df %>% mutate(mes_dia = format(TweetAt, "%m-%d"))
tweets_mes_dia %>% group_by(Sentiment, mes_dia) %>% summarise(n = n()) %>%
ggplot(aes(x = mes_dia, y = n, color = Sentiment)) +
geom_line(aes(group = Sentiment)) +
labs(title = "Número de tweets publicados", x = "fecha de publicación",
y = "número de tweets") +
theme_bw() +
theme(axis.text.x = element_text(angle = 90, size = 6),
legend.position = "bottom")
df %>% ggplot(aes(x = Sentiment)) + geom_bar(stat="count") + coord_flip() + theme_bw()
df %>% group_by(Sentiment) %>% summarise(Proporcion = n()/nrow(df)) %>% arrange(-Proporcion)
top_10 <- df %>% group_by(Location) %>% summarise(N=n()/nrow(df)) %>% arrange(-N)
top_10 <- top_10[1:10,]
top_10$N <- round(top_10$N,4)
ggplot(top_10,aes(x=reorder(Location,N), y =N )) +
geom_bar(stat="identity") +
coord_flip() +
labs(x="Pais", y = "Proporción del total de tweets", title="Top-10 locaciones con más tweets") +
theme_bw()
limpiar_texto <- function(texto){
# Se convierte todo el texto a minúsculas
nuevo_texto <- tolower(texto)
# Eliminación de páginas web (palabras que empiezan por "http." seguidas
# de cualquier cosa que no sea un espacio)
nuevo_texto <- str_replace_all(nuevo_texto,"http\\S*", "")
# Eliminación de signos de puntuación
nuevo_texto <- str_replace_all(nuevo_texto,"[[:punct:]]", " ")
# Eliminación de números
nuevo_texto <- str_replace_all(nuevo_texto,"[[:digit:]]", " ")
# Eliminación de espacios en blanco múltiples
nuevo_texto <- str_replace_all(nuevo_texto,"[\\s]+", " ")
# Tokenización por palabras individuales
nuevo_texto <- str_split(nuevo_texto, " ")[[1]]
# Eliminación de tokens con una longitud < 2
nuevo_texto <- keep(.x = nuevo_texto, .p = function(x){str_length(x) > 1})
return(nuevo_texto)
}
text = "Hola mi nombre es https://www.google.cl. Como. no sé xd6666 ASDA"
limpiar_texto(text)
tweets <- df %>% mutate(texto_vector = map(.x = OriginalTweet, .f = limpiar_texto))
tweets %>% select(texto_vector) %>% head()
tweets$texto_vector[1]
#unnest() nos permite realizar una expansión de los vectores de palabras que creamos, esto aumenta la dimension de filas considerablemente
tweets_expand <- tweets %>% select(-OriginalTweet) %>% unnest()
tweets_expand <- tweets_expand %>% rename(word = texto_vector)
head(tweets_expand)
#Utilizaremos la librería de stopwords para obtener palabras que no aportar información
library(stopwords)
# "word" %in% vector -> true or false
lista_stopwords <- stopwords("english")
lista_stopwords <- c(lista_stopwords, "amp","can","cant")
tweets_expand <- tweets_expand %>% filter(!(word %in% lista_stopwords))
tweets_expand %>% group_by(Sentiment, word) %>%
count(word) %>%
group_by(Sentiment) %>%
top_n(10,n) %>%
arrange(Sentiment, desc(n)) %>%
ggplot(aes(x=reorder(word,n),y=n,fill=Sentiment)) +
geom_col() +
labs(y = "Frecuencia", x = "Palabras mas repetidas") +
coord_flip()
pal_neg <- c("#FC9272", "#FB6A4A", "#EF3B2C", "#CB181D", "#A50F15")
pal_neu <- c("#A1D99B", "#74C476", "#41AB5D", "#238B45", "#006D2C")
pal_pos <- c("#9ECAE1", "#6BAED6", "#4292C6", "#2171B5", "#08519C")
top <- 400
#Palabras mas repetidas en comentarios negativos y extremadamente negativos
neg_tweets <- tweets_expand %>%
filter(Sentiment == "Negative" | Sentiment == "Extremely Negative") %>%
count(word)
neg_tweets_top <- neg_tweets%>%
arrange(desc(n)) %>%
top_n(top,n)
set.seed(1234)
wordcloud(words = neg_tweets_top$word, scale=c(5,0.7), freq = neg_tweets_top$n, min.freq = 1,max.words=100, random.order=FALSE, rot.per=0.35, colors=pal_neg)
#Palabras mas repetidas en comentarios neutrales
neu_tweets <- tweets_expand %>%
filter(Sentiment == "Neutral") %>%
count(word)
neu_tweets_top <- neu_tweets%>%
arrange(desc(n)) %>%
top_n(top,n)
set.seed(1234)
wordcloud(words = neu_tweets_top$word, scale=c(5,0.7), freq = neu_tweets_top$n, min.freq = 1,max.words=100, random.order=FALSE, rot.per=0.35, colors=pal_neu, fixed.asp = TRUE)
#Palabra mas repetidas en comentarios positivos y extremadamente positivos
pos_tweets <- tweets_expand %>%
filter(Sentiment == "Positive" | Sentiment == "Extremely Positive") %>%
count(word)
pos_tweets_top <- pos_tweets%>%
arrange(desc(n)) %>%
top_n(top,n)
set.seed(1234)
wordcloud(words = pos_tweets_top$word, scale=c(5,0.7), freq = pos_tweets_top$n, min.freq = 1,max.words=100, random.order=FALSE, rot.per=0.35, colors=pal_pos, fixed.asp = TRUE)
#Promedio en el largo de palabras por sentimiento.
require(stringr)
cat("Negative:",mean(str_length(neg_tweets$word)),"\n")
cat("Neutral:",mean(str_length(neu_tweets$word)),"\n")
cat("Positive:",mean(str_length(pos_tweets$word)),"\n")
#N-grams (?)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidytext)
#Utilizaremos la librería de stopwords para obtener palabras que no aportar información
library(stopwords)
train  <- read.csv("data/Corona_NLP_train.csv", encoding="Latin-1")
test <- read.csv("data/Corona_NLP_test.csv", encoding="Latin-1")
df <- rbind(train,test)
head(df)
df$TweetAt <- as.Date(df$TweetAt, format="%d-%m-%y")
df$OriginalTweet <- as.character(df$OriginalTweet)
str(df)
summary(df$TweetAt)
ggplot(data=df, aes(x=TweetAt)) + geom_histogram(position="identity", bins=30) +
labs(title = "Distribución de las fechas de tweets", x = "fecha de publicación",
y = "número de tweets") + theme_bw()
tweets_mes_dia <- df %>% mutate(mes_dia = format(TweetAt, "%m-%d"))
tweets_mes_dia %>% group_by(Sentiment, mes_dia) %>% summarise(n = n()) %>%
ggplot(aes(x = mes_dia, y = n, color = Sentiment)) +
geom_line(aes(group = Sentiment)) +
labs(title = "Número de tweets publicados", x = "fecha de publicación",
y = "número de tweets") +
theme_bw() +
theme(axis.text.x = element_text(angle = 90, size = 6),
legend.position = "bottom")
# library(tidyverse)
#tweets_sentiment <- df %>% group_by(Sentiment) %>% summarise(n = n())
df %>% ggplot(aes(x = Sentiment)) + geom_bar(stat="count") + coord_flip() + theme_bw()
df %>% group_by(Sentiment) %>% summarise(Proporcion = n()/nrow(df)) %>% arrange(-Proporcion)
top_10 <- df %>% group_by(Location) %>% summarise(N=n()/nrow(df)) %>% arrange(-N)
top_10 <- top_10[1:10,]
top_10$N <- round(top_10$N,4)
top_10
limpiar_texto <- function(texto){
# Se convierte todo el texto a minúsculas
nuevo_texto <- tolower(texto)
# Eliminación de páginas web (palabras que empiezan por "http." seguidas
# de cualquier cosa que no sea un espacio)
nuevo_texto <- str_replace_all(nuevo_texto,"http\\S*", "")
# Eliminación de signos de puntuación
nuevo_texto <- str_replace_all(nuevo_texto,"[[:punct:]]", " ")
# Eliminación de números
nuevo_texto <- str_replace_all(nuevo_texto,"[[:digit:]]", " ")
# Eliminación de espacios en blanco múltiples
nuevo_texto <- str_replace_all(nuevo_texto,"[\\s]+", " ")
# Tokenización por palabras individuales
nuevo_texto <- str_split(nuevo_texto, " ")[[1]]
# Eliminación de tokens con una longitud < 2
nuevo_texto <- keep(.x = nuevo_texto, .p = function(x){str_length(x) > 1})
return(nuevo_texto)
}
text = "Hola mi nombre es https://www.google.cl. Como. no sé xd6666 ASDA"
limpiar_texto(text)
tweets <- df %>% mutate(texto_vector = map(.x = OriginalTweet, .f = limpiar_texto))
tweets %>% select(texto_vector) %>% head()
#Cada valor de la columna texto_vector es un vector con cada palabara del textos
tweets$texto_vector[1]
#unnest() nos permite realizar una expansión de los vectores de palabras que creamos, esto aumenta la dimension de filas considerablemente
tweets_expand <- tweets %>% select(-OriginalTweet) %>% unnest()
tweets_expand <- tweets_expand %>% rename(word = texto_vector)
head(tweets_expand)
# "word" %in% vector -> true or false
lista_stopwords <- stopwords("english")
lista_stopwords <- c(lista_stopwords, "amp","can")
tweets_expand <- tweets_expand %>% filter(!(word %in% lista_stopwords))
tweets_expand %>% group_by(Sentiment, word) %>%
count(word) %>%
group_by(Sentiment) %>%
top_n(10,n) %>%
arrange(Sentiment, desc(n)) %>%
ggplot(aes(x=reorder(word,n),y=n,fill=Sentiment)) +
geom_col() +
labs(y = "Frecuencia", x = "Ppalabras mas repetidas") +
coord_flip()
require(RColorBrewer)
require(wordcloud)
require(wordcloud2)
top <- 400
#Palabras mas repetidas en comentarios negativos y extremadamente negativos
neg_tweets <- tweets_expand %>%
filter(Sentiment == "Negative" | Sentiment == "Extremely Negative") %>%
count(word)
neg_tweets_top <- neg_tweets%>%
arrange(desc(n)) %>%
top_n(top,n)
wordcloud(head(neg_tweets_top$word, 100), head(neg_tweets_top$n, 100), random.order=FALSE, colors=brewer.pal(8, "Dark2"))
wordcloud2(data=neg_tweets_top, size=1.6, color='random-dark')
#Palabras mas repetidas en comentarios neutrales
neu_tweets <- tweets_expand %>%
filter(Sentiment == "Neutral") %>%
count(word)
neu_tweets_top <- neu_tweets%>%
arrange(desc(n)) %>%
top_n(top,n)
wordcloud(head(neu_tweets_top$word, 100), head(neu_tweets_top$n, 100), random.order=FALSE, colors=brewer.pal(8, "Dark2"))
wordcloud2(data=neu_tweets_top, size=1.6, color='random-dark')
#Palabra mas repetidas en comentarios positivos y extremadamente positivos
pos_tweets <- tweets_expand %>%
filter(Sentiment == "Positive" | Sentiment == "Extremely Positive") %>%
count(word)
pos_tweets_top <- pos_tweets%>%
arrange(desc(n)) %>%
top_n(top,n)
wordcloud(head(pos_tweets_top$word, 100), head(pos_tweets_top$n, 100), random.order=FALSE, colors=brewer.pal(8, "Dark2"))
wordcloud2(data=pos_tweets_top, size=1.6, color='random-dark')
#Promedio en el largo de palabras por sentimiento.
require(stringr)
cat("Negative:",mean(str_length(neg_tweets$word)),"\n")
cat("Neutral:",mean(str_length(neu_tweets$word)),"\n")
cat("Positive:",mean(str_length(pos_tweets$word)),"\n")
#Revisar Hashtags mas populares
#N-grams (?)
ggplot(data, aes(x = division_idh , y = log(meat_production_tonnes) )) +
geom_boxplot() +
labs(title="Boxplot división IDH", x = "División IDH", y="Producción de carne[toneladas]") +
theme_bw()
#+coord_cartesian(xlim = c(0, 100), ylim = c(10, 20))
